{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I Softmax Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qsr1\n",
    "\n",
    "**(1) Show that probabilities sum to 1**\n",
    "\n",
    "$$P[y=1] = \\frac{1}{1+e^{-\\vec{w}\\cdot \\vec{x}}} = \\frac{e^{\\vec{w}\\cdot \\vec{x}}}{e^{\\vec{w}\\cdot \\vec{x}} + e^{\\vec{0} \\cdot \\vec{x}}}$$\n",
    "\n",
    "$$P[y=0] = 1 - \\frac{1}{1+e^{-\\vec{w}\\cdot \\vec{x}}} = \\frac{e^{\\vec{0} \\cdot \\vec{x}}}{e^{\\vec{w}\\cdot \\vec{x}} + e^{\\vec{0}\\cdot \\vec{x}}}$$\n",
    "\n",
    "$$P[y=i] = \\frac{e^{\\vec{w_i} \\cdot \\vec{x}}}{\\sum_j{e^{\\vec{w_j} \\cdot \\vec{x}}}}$$\n",
    "\n",
    "Summing all probabilities:\n",
    "\n",
    "$$\\sum_i{P[y=i]} = \\sum_i{\n",
    "    \\frac{e^{\\vec{w_i} \\cdot \\vec{x}}}{\\sum_j{e^{\\bar{w_j}\\cdot \\vec{x}} }}\n",
    "    =\n",
    "    \\frac{\n",
    "        \\sum_i{e^{\\vec{w_i} \\cdot \\vec{x}}}\n",
    "    }{\n",
    "        \\sum_j{e^{\\bar{w_j}\\cdot \\vec{x}} }    }\n",
    "    = 1\n",
    "}$$\n",
    "\n",
    "Proven."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2) What are the dimensions of $W$? $X$? $WX$?**\n",
    "\n",
    "$\\vec{x_i}$ is $n$ dimensional.\\\n",
    "Thus $\\vec{x_i}$ is a $(1 \\times n)$ matrix.\n",
    "\n",
    "Say we have $m$ classes.\\\n",
    "Then $W$ is a $(n \\times m)$ matrix.\n",
    "\n",
    "Say we have $p$ examples.\\\n",
    "Then $X$ is a $(p \\times n)$ matrix.\n",
    "\n",
    "Thus $WX$ is a $(p \\times m)$ matrix."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qsr2: code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qsr3\n",
    "\n",
    "In the cost function, we see the line\n",
    "\n",
    "```python\n",
    "W_X = W_X - np.max(W_X)\n",
    "```\n",
    "This means that each entry is reduced by the largest entry in the matrix."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1) Show that this does not affect the predicted probabilities.**\n",
    "\n",
    "We are given from above that probability is:\n",
    "\n",
    "$$P[y=i] = \\frac{e^{\\vec{w_i} \\cdot \\vec{x}}}{\\sum_j{e^{\\vec{w_j} \\cdot \\vec{x}}}}$$\n",
    "\n",
    "We can write that code line above as\n",
    "\n",
    "$$WX = WX - \\max{(WX)}$$\n",
    "\n",
    "Manipulating the probability equation:\n",
    "\n",
    "$$\n",
    "\\begin{align}P[y=i]\n",
    "&= \n",
    "\\frac{\n",
    "\\left( \\frac{1}{e^{\\max{(WX)}}} \\right)}\n",
    "{\\left( \\frac{1}{e^{\\max{(WX)}}} \\right)}\n",
    "\n",
    "\\frac{e^{\\vec{w_i} \\cdot \\vec{x}}}{\\sum_j{e^{\\vec{w_j} \\cdot \\vec{x}}}}\\\\\n",
    "&= \\frac{e^{\\vec{w_i} \\cdot \\vec{x} - \\max{(WX)}}}\n",
    "{\\sum_j{e^{\\vec{w_j} \\cdot \\vec{x} - \\max{(WX)}}}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Therefore, the probability is not affected by the subtraction of the maximum."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2) Why might this be an optimization over using W_X? Justify your answer.**\n",
    "\n",
    "Calculating any exponential can result in huge numbers, and may result in buffer overflow. Dividing a huge number is computationally difficult as well. The usage of $\\max(WX)$ is to reduce the size of the exponential, and thus reduce the computational cost."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qsr4\n",
    "**Use the `learningCurve` function in `runClassifier.py` to plot the accuracy of the classifier as a function of the number of examples seen. Include the plot in your write-up. Do you observe any overfitting or underfitting? Discuss and expain what you observe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from runClassifier import *\n",
    "#plot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II NN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qnn 1.4\n",
    "\n",
    "**When initializing the weight matrix, in some cases it may be appropriate to initialize the entries as small random numbers rather than all zeros.  Give one reason why this may be a good idea.**\n",
    "\n",
    "If all the weights are initialised to all zeros, then there will be 0 updates as the gradient descent algorithm will not update. This means that the weights will not change, and thus the algorithm will not learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 | packaged by conda-forge | (main, Mar 30 2022, 08:38:02) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "98909b20b47e7306ec03f5e42153095d0b5a272c9bf0befd3ab45b66b6487d01"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
